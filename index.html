---
layout: page
gh-repo: vskadandale/vocalist
gh-badge: [star, watch, fork, follow]
share-description: Official website of VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices
---
<div class="container">
    <div class="row">
        <div class="col-xl-12 mx-auto text-center">
            <h1>VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices</h1>
        </div>
        <div class="col-md-10 col-lg-8 col-xl-7 mx-auto">
        </div>
    </div>
</div>


<div class="col-xl-10 col-lg-8 offset-lg-1">

    <!-- Testimonials -->
    <section class="testimonials text-center">
        <div class="container">
            <div class="row">
                <div class="col-lg-4 text-center">
                    <div class="testimonial-item mx-auto mb-5 mb-lg-0">
                        <h5>
                            <a href="mailto:venkatesh.kadandaleATupfDOTedu"
                               style="text-decoration : none; color : #000000;">
                                Venkatesh S. Kadandale
                            </a>
                        </h5>
                        <p class="font-weight-light mb-0"></p>
                    </div>
                </div>
                <div class="col-lg-4 text-center">
                    <div class="testimonial-item mx-auto mb-5 mb-lg-0">
                        <h5>
                            <a href="mailto:juanfelipe.montesinosATupfDOTedu"
                               style="text-decoration : none; color : #000000;">
                                Juan F. Montesinos
                            </a>
                        </h5>
                        <p class="font-weight-light mb-0"></p>
                    </div>
                </div>
                <div class="col-lg-4 text-center">
                    <div class="testimonial-item mx-auto mb-5 mb-lg-0">
                        <h5>
                            <a href="mailto:gloria.haroATupfDOTedu" style="text-decoration : none; color : #000000;">
                                Gloria Haro
                            </a>
                        </h5>
                        <p class="font-weight-light mb-0"></p>
                    </div>
                </div>
            </div>
            <div class="row">
                <div class="offset-lg-3 col-lg-6 padtop" style="padding-bottom: 2rem">
          <span class="align-middle">
            <p class="mylead2">
                <a href="https://www.upf.edu/web/etic"
                style="color:black">Universitat Pompeu Fabra, Barcelona, Spain</a><br>

              
          </span>
                </div>
            </div>
        </div>
    </section>
    <div class="row justify-content-center">
        <div class="col-sm-3 text-center">
            <a target="_blank"
               href="https://arxiv.org/pdf/2204.02090.pdf"><img src="assets/img/paper.png" width="120" height="130" style="border:1px solid black;"></a>
            <h5 style="padding-bottom: 5%; padding-top: 5%">Paper</h5>
        </div>
        <div class="col-sm-3 text-center">
            <a href="https://github.com/vskadandale/vocalist"
               style="color: #242124">
                <i class="fab fa-github fa-8x"></i></a>
            <h5 style="padding-bottom: 5%; padding-top: 5%">Code + Weights</h5>
        </div>
        <div class="col-sm-3 text-center">
            <a style="color: #242124;"
               href="./demos/">
                <i class="fas fa-film fa-8x" style="transform: scale(1,1.275); padding-top: 2.5px"></i>
            </a>
            <h5 style="padding-bottom: 5%; padding-top: 5px">Demos</h5>
        </div>
    </div>
    <h6 class="mx-auto text-center" style="color: saddlebrown">The paper is under review.</h6>
    </br>
    <!-- Image Showcases -->
    <h2 style="text-align: center">Abstract</h2>
    <p class="lead mb-0" align="justify">
        In this paper, we address the problem of lip-voice synchronisation
        in videos containing human face and voice. Our approach is based on
        determining if the lips motion and the voice in a video are synchronised
        or not, depending on their audio-visual correspondence score. We propose
        an audio-visual cross-modal transformer-based model that outperforms
        several baseline models in the audio-visual synchronisation task on the
        standard lip-reading speech benchmark dataset LRS2. While the existing
        methods focus mainly on the lip synchronisation in speech videos, we also
        consider the special case of singing voice. Singing voice is a more
        challenging use case for synchronisation due to sustained vowel sounds.
        We also investigate the relevance of lip synchronisation models trained
        on speech datasets in the context of singing voice. Finally, we use the
        frozen visual features learned by our lip synchronisation model in the
        singing voice separation task to outperform a baseline audio-visual model
        which was trained end-to-end. The demos, source code and the pre-trained model
        will be made available on
        <a href="https://ipcv.github.io/VocaLiST/">https://ipcv.github.io/VocaLiST/</a>
    </p>
    </br>

    <div class="mx-auto">
        <br>
        <h5>Citation</h5>
        <pre class="hightlight" style="background-color:rgba(0,0,0, 0.1)"><p class="mb-0" align="justify">@article{kadandale2022vocalist,
  title={VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices},
  author={Kadandale, Venkatesh S and Montesinos, Juan F and Haro, Gloria},
  journal={arXiv preprint arXiv:2204.02090},
  year={2022}
}</p></pre>
    </div>

    <div class="mx-auto">
        <br>
        <h5>Acknowledgements</h5>
        <p class="lead mb-0" align="justify">
            We acknowledge support by MICINN/FEDER UE project
            PGC2018-098625-B-I00; H2020-MSCA-RISE-2017 project 777826 NoMADS.
            V. S. K. has received support through “la Caixa” Foundation (ID 100010434),
            fellowship code: LCF/BQ/DI18/11660064 and the Marie SkłodowskaCurie grant
            agreement No. 713673. J. F. M. acknowledges support by FPI
            scholarship PRE2018-083920.

            We would like to thank
            <a href="https://www.robots.ox.ac.uk/~hchen/">Honglie Chen</a> (VGG, Oxford) and
            <a href="https://soowhanchung.github.io">Soo-Whan Chung</a>
            (Naver Corporation) for the insightful discussions on the topic.
            </p>
    </div>

</div>
